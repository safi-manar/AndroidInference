import argparse
import os
import csv
import json

from pprint import pprint

from preprocessor import Preprocessor
from summarizer import Summarizer
import pandas

def parse_args():
    """Specify a parser for a single mandatory command-line argument"""
    parser = argparse.ArgumentParser(description='Preprocessing operations for CSVs generated by data-extract.py')
    parser.add_argument('extract_path', help='path to a single device\'s extracted data')
    parser.add_argument('-o', '--out-path', help='path to the output directory, defaults to in-place preprocessing if left unspecified')
    parser.add_argument('-s', '--spec-file', help='path to a custom preprocessing spec file')
    parser.add_argument('-m', '--merge', action='store_true')
    parser.add_argument('-t', '--timestep', type=int, help='the number of minutes in a data coverage timestep (currently not implemented)')

    return parser.parse_args()

def read_spec(spec_file):
    """Reads the data in the spec JSON file"""
    print 'spec_file=%s' % spec_file
    with open(spec_file) as json_file:
        json_data = json.load(json_file)

        return json_data

# Read arguments
script_path = os.path.dirname(os.path.realpath(__file__))
default_spec_file = os.path.join(script_path, 'spec.json')
cols_key = 'primary-cols'
rename_key = 'rename'

args = parse_args()
extract_path = args.extract_path
out_path = os.path.abspath(args.out_path) if args.out_path is not None else extract_path
spec_file = args.spec_file if args.spec_file is not None else default_spec_file
to_merge = args.merge

print 'extract_path="%s"' % extract_path
print 'out_path="%s"' % out_path

if(not os.path.isdir(out_path)):
    os.makedirs(out_path)

# Read spec
spec = read_spec(spec_file)
target_files = [val for val in spec.keys() if val.endswith('.csv')]
sensor_keys = spec['meta']['sensormerge'].split(',')
all_keys = spec['meta']['allmerge'].split(',')

# Preprocess and summarize extracted data
extracted_files = [f for f in os.listdir(extract_path) if os.path.isfile(os.path.join(extract_path, f))]
to_preprocess = [f for f in extracted_files if f in target_files]
to_ignore = [f for f in extracted_files if f not in target_files]

sensormerge = []
othermerge = []
summary = {}
for target in to_preprocess:
    target_path = os.path.join(extract_path, target)
    columns = spec[target][cols_key].split(',')
    renames = spec[target][rename_key] if rename_key in spec[target].keys() else None

    # Preprocess and write out a standalone file for the data source
    preproced = Preprocessor(target_path, columns, renames).run()
    if to_merge:
        if set(sensor_keys) < set(columns):
            sensormerge.append(preproced)
        else:
            othermerge.append(preproced)
    preproced.to_csv(os.path.join(out_path, target + '.pprc'), header=preproced.columns, index=False)

    # Generate summary of preprocessed data
    summary[target] = Summarizer(preproced).get_full_report()

# Note missing files and write out a standalone summary file
summary['missing'] = [f for f in target_files if f not in to_preprocess]
with open(os.path.join(out_path, 'summary.json'), 'w') as sumfile:
    json.dump(summary, sumfile, sort_keys=True, indent=4)

# Merge sensors together first, then everything else, then write it out
if to_merge:
    merged = reduce(lambda x,y: x.merge(y, how='outer', on=sensor_keys, sort=False), sensormerge)
    merged = reduce(lambda x,y: x.merge(y, how='outer', on=all_keys, sort=True), othermerge, merged)
    merged.to_csv(os.path.join(out_path, 'merged.pprc'), header=merged.columns, index=False)

for ignore in to_ignore:
    print 'Ignored %s' % ignore
